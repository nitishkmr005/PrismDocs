{
  "title": "Tensor Parallelism for Multi-GPU Transformer Inference",
  "visual_markers": [
    {
      "marker_id": "visual_0",
      "type": "comparison",
      "title": "Title",
      "description": "Parallelism Strategies Comparison:A table comparing Data, Pipeline, and Tensor Parallelism based on what is split, memory per GPU, and communication overhead.",
      "position": 2411
    },
    {
      "marker_id": "visual_1",
      "type": "concept_map",
      "title": "Title",
      "description": "Column-Parallel Matrix Multiplication:Illustration of splitting a weight matrix W by its columns and performing parallel matrix multiplications without immediate communication.",
      "position": 3618
    },
    {
      "marker_id": "visual_2",
      "type": "concept_map",
      "title": "Title",
      "description": "Row-Parallel Matrix Multiplication:Illustration of multiplying a column-sharded input X with a row-split weight matrix W, followed by an all-reduce operation to sum the results.",
      "position": 4300
    },
    {
      "marker_id": "visual_3",
      "type": "flowchart",
      "title": "Title",
      "description": "Tensor Parallelism in an Attention Layer:Flow showing column-parallel QKV projections, independent local attention computation, and row-parallel output projection with a final all-reduce operation.",
      "position": 5160
    }
  ],
  "outline": "```json\n{\n  \"outline\": \"# Tensor Parallelism for Multi-GPU Transformer Inference\\n## Outline\\n1. Introduction\\n2. The Need for Tensor Parallelism\\n   2.1 The Memory Wall Problem with Large Models\\n   2.2 Comparing Parallelism Strategies\\n3. How Tensor Parallelism Works\\n   3.1 Column-Parallel Matrix Multiplication\\n   3.2 Row-Parallel Matrix Multiplication\\n4. Applying TP in Transformer Layers\\n   4.1 Tensor Parallelism in the Attention Layer\\n   4.2 Tensor Parallelism in the Feed-Forward Network\\n   4.3 Communication and Practical Constraints\\n5. Implementing TP with HuggingFace Transformers\\n   5.1 A Simple Code Implementation\\n   5.2 Launching with torchrun\\n   5.3 Supported Models and Partitioning Strategies\\n6. Hands-On Guide: Tensor Parallelism on RunPod\\n   6.1 Setting Up a Multi-GPU Pod\\n   6.2 Environment Setup and Inference Script\\n   6.3 Launching the Inference Job\\n   6.4 Using vLLM with Tensor Parallelism\\n7. Hands-On Guide: Tensor Parallelism on Lambda Cloud\\n   7.1 Launching a Multi-GPU Instance\\n   7.2 Creating and Launching the Inference Script\\n   7.3 Multi-Node Setup for Large-Scale Inference\\n8. Performance Benchmarks and Observations\\n   8.1 Llama-3-70B Inference Throughput\\n   8.2 Key Performance Scaling Observations\\n9. Limitations and Advanced Strategies\\n   9.1 What Tensor Parallelism Does Not Solve\\n   9.2 Combining with Pipeline Parallelism\\n10. Key Takeaways\"\n}\n```",
  "markdown": "# Tensor Parallelism for Multi-GPU Transformer Inference\n\n## Introduction\n\nRunning large language models, such as a 70-billion parameter model, on a single GPU is often infeasible due to memory limitations. Even a high-end H100 GPU with 80GB of VRAM cannot accommodate a model like Llama-2-70B in full precision. Tensor Parallelism (TP) addresses this challenge by splitting a model's weight matrices across multiple GPUs, enabling the execution of models that would otherwise be too large. This guide provides a hands-on approach to understanding and implementing tensor-parallel inference, covering the underlying theory before diving into practical code examples for running inference on cloud platforms like RunPod and Lambda Cloud.\n\n## 1. Introduction\n\nThis guide focuses on the practical application of Tensor Parallelism for large model inference. We will cover the necessary theoretical concepts to understand its mechanics and then proceed with implementation details. By the conclusion, you will have functional scripts for executing tensor-parallel inference on multi-GPU environments.\n\n## 2. The Need for Tensor Parallelism\n\nThe primary driver for using parallelism strategies is the immense size of modern large language models, which creates a significant memory bottleneck.\n\n### 2.1 The Memory Wall Problem with Large Models\n\nModern LLMs require substantial GPU memory, often exceeding the capacity of a single device. The memory footprint grows directly with the number of parameters and the precision used (e.g., FP16 or FP32).\n\n| Model        | Parameters   | FP16 Memory   | FP32 Memory   |\n|--------------|--------------|---------------|---------------|\n| Llama-3-8B   | 8B           | ~16 GB        | ~32 GB        |\n| Llama-3-70B  | 70B          | ~140 GB       | ~280 GB       |\n| Llama-3-405B | 405B         | ~810 GB       | ~1.6 TB       |\n\nAs the table illustrates, a single A100 GPU with 80GB of VRAM can barely hold the Llama-3-70B model in FP16, and this does not account for the additional memory required for the KV cache, activations, and batch size overhead. To run larger models, it is essential to distribute the model across multiple GPUs.\n\n### 2.2 Comparing Parallelism Strategies\n\nSeveral strategies exist for distributing computational workloads across multiple GPUs, each with distinct characteristics regarding what is split, memory usage, and communication patterns.\n\n[VISUAL:comparison:Title:Parallelism Strategies Comparison:A table comparing Data, Pipeline, and Tensor Parallelism based on what is split, memory per GPU, and communication overhead.]\n\n| Strategy             | What’s Split    | Memory per GPU         | Communication Overhead      |\n|----------------------|-----------------|------------------------|-----------------------------|\n| Data Parallelism     | Data batches    | Full model on each GPU | Gradient sync after each step |\n| Pipeline Parallelism | Layers          | Subset of layers       | Activations between stages    |\n| Tensor Parallelism   | Weight matrices | Slice of every layer   | All-reduce within each layer  |\n\nTensor Parallelism is the preferred strategy when the model is too large to fit on a single GPU, when fast interconnects like NVLink or InfiniBand are available, and when the primary goal is to minimize inference latency.\n\n## 3. How Tensor Parallelism Works\n\nThe fundamental concept behind Tensor Parallelism is that matrix multiplications, which are core operations in Transformers, can be parallelized by splitting the matrices themselves across different processing units.\n\n### 3.1 Column-Parallel Matrix Multiplication\n\n[VISUAL:concept_map:Title:Column-Parallel Matrix Multiplication:Illustration of splitting a weight matrix W by its columns and performing parallel matrix multiplications without immediate communication.]\n\nConsider the computation `Y = XW`, where `X` is the input and `W` is a weight matrix. In a column-parallel approach, the weight matrix `W` is split into column blocks. Each GPU computes its portion of the output by multiplying the full input `X` with its assigned column slice of `W`. The resulting outputs `Y_i` are also sharded by columns. This step can be performed independently on each GPU without any inter-GPU communication.\n\n### 3.2 Row-Parallel Matrix Multiplication\n\n[VISUAL:concept_map:Title:Row-Parallel Matrix Multiplication:Illustration of multiplying a column-sharded input X with a row-split weight matrix W, followed by an all-reduce operation to sum the results.]\n\nNow, if the input `X` is already sharded by columns, as in `X = [X_1 | X_2 | ... | X_n]`, and the weight matrix `W` is split into corresponding row blocks, each GPU can compute a partial result. To obtain the final output `Y`, the partial results from all GPUs must be summed together. This is achieved through an **all-reduce** communication operation, which is a key step requiring GPU-to-GPU data transfer.\n\n## 4. Applying TP in Transformer Layers\n\nTensor Parallelism can be systematically applied to the main components of a Transformer layer: the attention mechanism and the feed-forward network.\n\n### 4.1 Tensor Parallelism in the Attention Layer\n\n[VISUAL:flowchart:Title:Tensor Parallelism in an Attention Layer:Flow showing column-parallel QKV projections, independent local attention computation, and row-parallel output projection with a final all-reduce operation.]\n\nThe attention mechanism involves three projection matrices for queries (`W_Q`), keys (`W_K`), and values (`W_V`), along with an output projection matrix (`W_O`). TP is applied in three steps.\n\n**Step 1: Split Q, K, V Projections (Column-Parallel)**\nThe `W_Q`, `W_K`, and `W_V` weight matrices are split by columns. This is equivalent to distributing the attention heads across the available GPUs. For instance, with 32 attention heads and 4 GPUs, each GPU would handle 8 heads. Each GPU computes its slice independently.\n```\nGPU i computes (column slices of weight matrices):\nQ_i = X × W_Q[all_rows, columns_for_heads_i]\nK_i = X × W_K[all_rows, columns_for_heads_i]\nV_i = X × W_V[all_rows, columns_for_heads_i]\n```\nThis stage requires no communication.\n\n**Step 2: Local Attention Computation**\nSince attention heads operate independently, each GPU can compute the attention mechanism for its assigned heads locally without communicating with other GPUs.\n```\nGPU i computes attention locally:\nattn_i = softmax(Q_i × K_i^T / √d_k) × V_i\n```\n**Step 3: Output Projection (Row-Parallel)**\nThe output projection matrix `W_O` is split by rows. Each GPU multiplies its local attention output by its slice of `W_O` to get a partial result. An all-reduce operation is then performed to sum these partial results into the final output.\n```\nGPU i computes partial output (row slice of W_O):\npartial_i = attn_i × W_O[rows_for_gpu_i, all_cols]\n\nAll GPUs synchronize:\noutput = AllReduce(partial_0 + partial_1 + ... + partial_n)\n```\nThis process results in one all-reduce operation per attention layer.\n\n### 4.2 Tensor Parallelism in the Feed-Forward Network\n\nThe feed-forward network (FFN) in a Transformer typically consists of two linear layers. TP is applied to these layers as follows:\n\n**First Linear Layer (Column-Parallel)**\nThe first linear layer's weight matrix, `W1`, is split by columns. Each GPU computes a partial hidden state independently.\n```\nGPU i computes:\nhidden_i = GELU(x × W1[all_rows, cols_for_gpu_i])\n```\n**Second Linear Layer (Row-Parallel)**\nThe second linear layer's weight matrix, `W2`, is split by rows. Each GPU computes a partial output using its hidden state slice. These partial outputs are then summed across all GPUs using an all-reduce operation.\n```\nGPU i computes:\npartial_i = hidden_i × W2[rows_for_gpu_i, all_cols]\n\nAll GPUs synchronize:\noutput = AllReduce(sum of all partial_i)\n```\nThis adds a second all-reduce operation, bringing the total to two all-reduce operations per complete Transformer layer.\n\n### 4.3 Communication and Practical Constraints\n\nIn total, a standard Transformer layer requires **two all-reduce operations** when using Tensor Parallelism. However, there are practical constraints to consider:\n\n1.  The TP size (number of GPUs) cannot exceed the number of attention heads.\n2.  The number of attention heads must be divisible by the TP size to ensure an equal workload on each GPU.\n3.  The FFN hidden dimension must also be divisible by the TP size.\n\nFor example, the Llama-3-70B model has 64 attention heads. Therefore, valid TP sizes are 1, 2, 4, 8, 16, 32, and 64.\n\n## 5. Implementing TP with HuggingFace Transformers\n\nThe HuggingFace `transformers` library offers built-in support for Tensor Parallelism, simplifying its implementation for many models to just a single parameter.\n\n### 5.1 A Simple Code Implementation\n\nFor supported models, enabling Tensor Parallelism can be done by setting the `tp_plan` argument to `\"auto\"` when loading the model. This instructs the library to automatically partition the model's weights across the available GPUs.\n```python\n# tp_inference.py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    torch_dtype=torch.bfloat16,\n    tp_plan=\"auto\"  # <-- This enables tensor parallelism\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n\nprompt = \"Explain tensor parallelism in one paragraph:\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=100)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n### 5.2 Launching with torchrun\n\nA script using Tensor Parallelism cannot be run directly with `python`. Instead, it must be launched using `torchrun`, which spawns multiple processes and manages the distributed environment. Each process is assigned to a separate GPU.\n\nTo run the script on 4 GPUs:\n```bash\ntorchrun --nproc-per-node 4 tp_inference.py\n```\n\n### 5.3 Supported Models and Partitioning Strategies\n\nAs of late 2025, HuggingFace provides native TP support for a growing list of models, including Llama, Mistral, Mixtral, Qwen, and Gemma. You can check if a model is supported by inspecting its configuration for a `_tp_plan` attribute.\n```python\nfrom transformers import AutoConfig\n\nconfig = AutoConfig.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\nprint(config._tp_plan)  # Shows the default TP plan\n```\nInternally, HuggingFace uses several partitioning strategies:\n\n| Strategy          | Description                               |\n|-------------------|-------------------------------------------|\n| `colwise`           | Column-parallel (for Q, K, V projections) |\n| `rowwise`           | Row-parallel (for output projections)     |\n| `sequence_parallel` | For LayerNorm, Dropout                    |\n| `replicate`         | Keep full copy on each GPU                |\n\nFor advanced use cases, you can define a custom `tp_plan` dictionary to specify how each layer should be partitioned.\n```python\ntp_plan = {\n    \"model.layers.*.self_attn.q_proj\": \"colwise\",\n    \"model.layers.*.self_attn.k_proj\": \"colwise\",\n    \"model.layers.*.self_attn.v_proj\": \"colwise\",\n    \"model.layers.*.self_attn.o_proj\": \"rowwise\",\n    \"model.layers.*.mlp.gate_proj\": \"colwise\",\n    \"model.layers.*.mlp.up_proj\": \"colwise\",\n    \"model.layers.*.mlp.down_proj\": \"rowwise\",\n}\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    torch_dtype=torch.bfloat16,\n    tp_plan=tp_plan\n)\n```\n\n## 6. Hands-On Guide: Tensor Parallelism on RunPod\n\nRunPod provides on-demand multi-GPU instances suitable for running large models with Tensor Parallelism. This section outlines the steps to run Llama-3-70B on a RunPod pod.\n\n### 6.1 Setting Up a Multi-GPU Pod\n\nFirst, deploy a new pod on RunPod:\n1.  Navigate to **RunPod → Pods → Deploy**.\n2.  Select a suitable PyTorch template, such as `runpod/pytorch:2.1.0-py3.10-cuda11.8.0`.\n3.  Choose a multi-GPU configuration, like 4x A100 80GB for Llama-3-70B or 8x H100 for larger models.\n\n**Critical**: It is essential to select instances with **NVLink** interconnects (e.g., SXM variants like A100-SXM or H100-SXM) rather than PCIe. NVLink offers 600-900 GB/s of inter-GPU bandwidth, whereas PCIe is limited to approximately 64 GB/s. The high communication cost of `all-reduce` operations makes NVLink crucial for TP performance; without it, the communication overhead becomes a severe bottleneck and negates performance gains.\n\n### 6.2 Environment Setup and Inference Script\n\nOnce connected to the pod via SSH, set up the environment by installing the necessary libraries.\n```bash\n# Update and install dependencies\npip install --upgrade transformers accelerate torch\n\n# Verify GPU setup\nnvidia-smi\n\n# Check NCCL (the communication backend)\npython -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}\\nGPU count: {torch.cuda.device_count()}')\"\n```\nThe expected output should confirm CUDA availability and list the correct number of GPUs. Next, create the inference script:\n```python\n# runpod_tp_inference.py\nimport os\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef main():\n    model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n\n    # Load model with tensor parallelism\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        torch_dtype=torch.bfloat16,\n        tp_plan=\"auto\"\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    # Only rank 0 should print\n    rank = int(os.environ.get(\"RANK\", 0))\n\n    prompts = [\n        \"What is tensor parallelism?\",\n        \"Explain the difference between data and model parallelism.\",\n    ]\n\n    for prompt in prompts:\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=200,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.9,\n            )\n\n        if rank == 0:\n            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            print(f\"\\n{'='*50}\")\n            print(f\"Prompt: {prompt}\")\n            print(f\"Response: {response}\")\n            print(f\"{'='*50}\\n\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### 6.3 Launching the Inference Job\n\nTo run the script, first set your HuggingFace token as an environment variable, then use `torchrun` to launch the job on all available GPUs.\n```bash\n# Set your HuggingFace token for gated models\nexport HF_TOKEN=\"your_token_here\"\n\n# Launch on 4 GPUs\ntorchrun --nproc-per-node 4 runpod_tp_inference.py\n```\n\n### 6.4 Using vLLM with Tensor Parallelism\n\nFor production-level inference, vLLM is often a more performant option. To use vLLM with tensor parallelism, you can run its OpenAI-compatible API server:\n```bash\n# Install vLLM\npip install vllm\n\n# Run with tensor parallelism\npython -m vllm.entrypoints.openai.api_server \\\n    --model meta-llama/Meta-Llama-3-70B-Instruct \\\n    --tensor-parallel-size 4 \\\n    --dtype bfloat16 \\\n    --port 8000\n```\nAlternatively, RunPod's serverless platform can automatically handle vLLM and TP configuration through a handler.\n```python\n# In your RunPod serverless handler\nhandler_config = {\n    \"model_name\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n    \"tensor_parallel_size\": 4,\n    \"dtype\": \"bfloat16\",\n}\n```\n\n## 7. Hands-On Guide: Tensor Parallelism on Lambda Cloud\n\nLambda Cloud provides high-performance GPU instances, including configurations with up to 8x H100 GPUs, making it another excellent choice for large model inference.\n\n### 7.1 Launching a Multi-GPU Instance\n\nTo start, launch a multi-GPU instance from the Lambda Cloud dashboard:\n1.  Navigate to **Lambda Cloud → Instances → Launch**.\n2.  Select an appropriate instance type, such as `gpu_8x_h100_sxm5` or `gpu_4x_a100_80gb_sxm4`.\n\n**Critical**: As with RunPod, always choose **SXM** variants (e.g., `sxm4`, `sxm5`) over PCIe. The SXM designation confirms that the GPUs are interconnected with high-bandwidth NVLink (600-900 GB/s), which is essential for efficient tensor parallelism. PCIe-based instances are limited to the CPU's PCIe lane bandwidth (~64 GB/s), creating a bottleneck that severely degrades performance.\n\n### 7.2 Creating and Launching the Inference Script\n\nAfter SSHing into the instance and completing the environment setup, create the following script, which includes simple benchmarking to measure performance.\n```python\n# lambda_tp_inference.py\nimport os\nimport time\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef main():\n    model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n    rank = int(os.environ.get(\"RANK\", 0))\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n\n    if rank == 0:\n        print(f\"Loading {model_id} with TP across {world_size} GPUs...\")\n\n    start_time = time.time()\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        torch_dtype=torch.bfloat16,\n        tp_plan=\"auto\"\n    )\n    if rank == 0:\n        load_time = time.time() - start_time\n        print(f\"Model loaded in {load_time:.2f}s\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    # Benchmark inference\n    prompt = \"Write a short poem about distributed computing:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    # Warmup\n    with torch.no_grad():\n        _ = model.generate(**inputs, max_new_tokens=10)\n\n    # Timed generation\n    torch.cuda.synchronize()\n    start = time.time()\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=100,\n            do_sample=False,  # Greedy for reproducibility\n        )\n    torch.cuda.synchronize()\n    gen_time = time.time() - start\n\n    if rank == 0:\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        tokens_generated = outputs.shape[1] - inputs[\"input_ids\"].shape[1]\n        tokens_per_sec = tokens_generated / gen_time\n\n        print(f\"\\nPrompt: {prompt}\")\n        print(f\"Response: {response}\")\n        print(f\"\\n--- Performance ---\")\n        print(f\"Tokens generated: {tokens_generated}\")\n        print(f\"Time: {gen_time:.2f}s\")\n        print(f\"Throughput: {tokens_per_sec:.1f} tokens/sec\")\n\nif __name__ == \"__main__\":\n    main()\n```\nLaunch the script using `torchrun`, specifying the number of GPUs on the node.\n```bash\n# For a single node with 4 GPUs\ntorchrun --nproc-per-node 4 lambda_tp_inference.py\n\n# For 8 GPUs\ntorchrun --nproc-per-node 8 lambda_tp_inference.py\n```\n\n### 7.3 Multi-Node Setup for Large-Scale Inference\n\nFor models that require more than 8 GPUs, you can extend Tensor Parallelism across multiple nodes using `torchrun`'s multi-node capabilities. This requires a high-bandwidth interconnect like InfiniBand between nodes to be effective.\n\nOn the master node (Node 0):\n```bash\ntorchrun \\\n    --nproc-per-node 8 \\\n    --nnodes 2 \\\n    --node-rank 0 \\\n    --master-addr <master-ip> \\\n    --master-port 29500 \\\n    lambda_tp_inference.py\n```\nOn the worker node (Node 1):\n```bash\ntorchrun \\\n    --nproc-per-node 8 \\\n    --nnodes 2 \\\n    --node-rank 1 \\\n    --master-addr <master-ip> \\\n    --master-port 29500 \\\n    lambda_tp_inference.py\n```\nThis configuration creates a 16-GPU setup with tensor parallelism. However, be aware that cross-node communication overhead can significantly impact performance if the interconnect is not sufficiently fast.\n\n## 8. Performance Benchmarks and Observations\n\nPerformance with Tensor Parallelism scales with the number of GPUs, but not perfectly due to communication overhead.\n\n### 8.1 Llama-3-70B Inference Throughput\n\nThe following table shows approximate inference throughput for Llama-3-70B on different H100 GPU configurations.\n\n| Configuration   | TP Size | Tokens/sec | Memory/GPU |\n|-----------------|---------|------------|------------|\n| 1× H100 80GB    | 1       | OOM        | -          |\n| 2× H100 80GB    | 2       | ~45        | ~38 GB     |\n| 4× H100 80GB    | 4       | ~85        | ~20 GB     |\n| 8× H100 80GB    | 8       | ~140       | ~12 GB     |\n\n### 8.2 Key Performance Scaling Observations\n\nBased on these benchmarks, several key patterns emerge:\n1.  **Memory scales linearly**: Using four times the number of GPUs reduces the memory requirement per GPU by approximately four times.\n2.  **Throughput scales sub-linearly**: Performance gains diminish as the TP size increases because the communication overhead from `all-reduce` operations becomes more significant.\n3.  **There is a sweet spot**: For many models, the optimal balance between performance and cost is often found with a TP size of 4 to 8 GPUs. Beyond this, communication can start to dominate computation time.\n\n## 9. Limitations and Advanced Strategies\n\nWhile powerful, Tensor Parallelism has inherent limitations and is often combined with other strategies for very large-scale systems.\n\n### 9.1 What Tensor Parallelism Does Not Solve\n\nTensor Parallelism has three main limitations:\n1.  **Scalability is Capped by Attention Heads**: The TP size cannot be larger than the model's number of attention heads. For a model with 64 heads, you cannot use more than 64 GPUs for TP.\n2.  **Communication Overhead Across Nodes**: TP requires frequent communication. This is fast within a node using NVLink (900 GB/s), but becomes a bottleneck across nodes connected by InfiniBand (~400 GB/s) or Ethernet (~100 Gbps).\n3.  **Activation Memory**: TP primarily reduces the memory required for model weights, but it does not reduce the memory needed for activations. For very long sequences, activation memory can still become a bottleneck.\n\n### 9.2 Combining with Pipeline Parallelism\n\nFor extremely large models (400B+ parameters), a hybrid approach combining Tensor Parallelism (TP) and Pipeline Parallelism (PP) is common. The general rule of thumb is to use TP within a single node to leverage fast NVLink interconnects and use PP across different nodes.\n\nFor example, to run a model on 32 GPUs, you could use an 8-way TP within each of four nodes, and a 4-way PP across the nodes:\n```\nNode 0: Layers 0-19  (TP=8 within node)\nNode 1: Layers 20-39 (TP=8 within node)\nNode 2: Layers 40-59 (TP=8 within node)\nNode 3: Layers 60-79 (TP=8 within node)\n```\nThis hybrid strategy effectively scales to a large number of GPUs by minimizing slow cross-node communication.\n\n## 10. Key Takeaways\n\nTensor parallelism is an essential technique for running large language models that do not fit on a single GPU. The core principles involve splitting the model's weight matrices across multiple GPUs—using a column-wise split for projection layers and a row-wise split for output layers—and using all-reduce operations to aggregate partial results. A standard Transformer layer requires two such communication steps. For optimal performance, it is best to confine Tensor Parallelism within a single, NVLink-connected node. The HuggingFace `transformers` library simplifies implementation with its `tp_plan=\"auto\"` feature. For production environments, frameworks like vLLM provide highly optimized TP implementations, while for training, FSDP (Fully Sharded Data Parallel) offers an alternative that combines elements of both tensor and data parallelism.\n\n## Key Takeaways\n\nTensor Parallelism is the go-to technique for running models that don’t fit on a single GPU. The key ideas are splitting weight matrices across GPUs (column-wise for projections, row-wise for outputs) and using all-reduce to aggregate partial results, with two such operations per transformer layer. For best performance, TP should be kept within a single node with fast interconnects like NVLink. For ease of use, HuggingFace's `tp_plan=\"auto\"` is a straightforward solution, while specialized frameworks like vLLM are recommended for production inference.",
  "sections": [
    "1. Introduction",
    "2. The Need for Tensor Parallelism",
    "3. How Tensor Parallelism Works",
    "4. Applying TP in Transformer Layers",
    "5. Implementing TP with HuggingFace Transformers",
    "6. Hands-On Guide: Tensor Parallelism on RunPod",
    "7. Hands-On Guide: Tensor Parallelism on Lambda Cloud",
    "8. Performance Benchmarks and Observations",
    "9. Limitations and Advanced Strategies",
    "10. Key Takeaways"
  ],
  "content_hash": "cd46b40e843c368b12c3c5614b7e467ca6170b115e5a175c64158f24a7579c24",
  "executive_summary": "*   State-of-the-art AI models now require more memory (e.g., ~140GB for Llama-3-70B) than a single high-end GPU can provide, creating a critical infrastructure bottleneck for deployment.\n*   Tensor Parallelism (TP) is the strategic solution to this memory limitation, enabling the execution of massive models by splitting their core weight matrices across multiple GPUs.\n*   When a model is too large for one GPU, TP is the preferred strategy for minimizing inference latency, which is crucial for the performance of real-time applications.\n*   TP strategically divides Transformer layer computations into independent parallel steps and synchronized steps that require data transfer between GPUs to combine results.\n*   The implementation introduces a manageable performance cost, with each Transformer layer requiring two communication-intensive \"all-reduce\" operations, making high-speed GPU interconnects essential."
}